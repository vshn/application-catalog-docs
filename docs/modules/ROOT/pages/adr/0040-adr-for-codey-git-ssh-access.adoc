= ADR 0040 - Codey Git SSH Access
:adr_author:    Marco De Luca
:adr_owner:     Schedar
:adr_reviewers: Schedar
:adr_date:      2025-10-01
:adr_upd_date:  2025-10-01
:adr_status:    draft
:adr_tags:      codey

include::partial$adr-meta.adoc[]

[NOTE]
.Summary
====
This ADR evaluates methods to enable Git over SSH for Codey instances and compares architectures for multiplexing SSH traffic across hundreds of tenants.
====

== Context

Codey users need a secure way to push and pull Git repositories using SSH keys.
The SSH protocol lacks SNI (Server Name Indication), which means hostname-based routing on a single port is not possible.
To multiplex multiple tenants through a single IP address, each customer instance must be assigned a unique TCP port.

Cloud provider LoadBalancers impose hard limits on the number of listeners (ports) per IP address:

* Cloudscale: up to 100 listeners per LoadBalancer
* Exoscale: up to 10 listeners per LoadBalancer


=== Considered Options

==== Option 1: Per-tenant LoadBalancer (distinct IP per instance)

Each Codey customer's Forgejo instance would expose its SSH Service directly as `type: LoadBalancer`, letting the cloud provider allocate a public IP and open TCP/22 for that instance.
This is the most straightforward design and uses Kubernetes native Service abstraction without any shared proxying layer.
Network traffic from the Internet terminates directly on each tenant's Forgejo pod, no intermediate routing or multiplexing is required.
Operationally, this model scales linearly: every new customer gets one LoadBalancer and one public IP.

Pros::
* Simple datapath
* Simpler to operate
* No lateral movement risk

Cons::
* Potentially hundreds of IPs
* Linear cost
* Violates consolidation goal


==== Option 2: One LB + one IP + file-based TCP proxy (graceful reloads)

A single shared LoadBalancer IP would forward all inbound SSH traffic to a TCP proxy Deployment (for example HAProxy).
The proxy configuration file would define one `bind :PORT` and `server <tenant>` pair per customer instance.
A custom controller would maintain that configuration file (for example a ConfigMap) and trigger a graceful reload whenever tenants are added or removed.
During reloads, HAProxy can keep existing sessions alive but still incurs short pauses for new connections.

Pros:: 
* Simple architecture
* Portable

Cons::
* Reloads can interrupt active sessions, altough sessions are short-lived for Git
* Config management complexity


==== Option 3: Dynamic proxy with runtime API (HAProxy Data Plane API / Envoy xDS)

Instead of editing files, the controller would communicate with the proxy at runtime:

  - For **HAProxy**, through its REST-based Data Plane API to add or remove frontends/backends dynamically.
  - For **Envoy**, through an xDS control-plane that updates listeners and clusters live.

The proxy stays running continuously. New tenants are added or removed atomically without restarting the process.
The controller becomes a lightweight "routing brain" maintaining the mapping of `(tenant → port → backend service)` and pushing those changes to the proxy's API.

Pros::
* Atomic updates
* No dropped connections
* Mature APIss

Cons::
* Needs custom control-plane component
* Higher operational burden

References::
* https://www.haproxy.com/documentation/haproxy-data-plane-api/[HAProxy Data Plane API]
* https://www.envoyproxy.io/docs/envoy/latest/api-docs/xds_protocol[Envoy xDS protocol]
* https://github.com/envoyproxy/go-control-plane[xDS go-control-plane]

==== Option 4: Kubernetes Gateway API (TCPRoute per tenant)

In this model, Codey uses the Kubernetes Gateway API as the control plane for multiplexing SSH traffic, independent of any specific proxy implementation.  
A conformant Gateway controller such as Envoy Gateway, Cilium Gateway, or KGateway manages a shared data plane that acts as a TCP multiplexer for all Codey customer SSH connections.

A Kubernetes `Gateway` resource represents the public entry point (one `Service type=LoadBalancer` with a single external IP).  
Each customer tenant receives a dedicated TCP `listener` bound to a unique port.  
For larger deployments, additional ports can be defined through `XListenerSet` (an experimental feature as the X implies) resources, which extend a `Gateway` with extra listener definitions beyond its native limit.

A custom controller acts as a lightweight orchestrator: it allocates a free port, creates a `TCPRoute` pointing to the tenant's internal Forgejo SSH `Service`, and adds a `ReferenceGrant` in the tenant's namespace to authorize the cross-namespace reference.
The selected Gateway implementation (Envoy, Cilium, etc.) then automatically reconciles these CRDs into live configuration, handling listener creation, routing, and backend resolution without reloads or direct API calls.

Pros::
* Fully Kubernetes-native and declarative, integrates cleanly with OpenShift and future Gateway API implementations.  
* Zero-downtime dynamic updates. Gateway controllers apply config atomically without restarts.  
* Simpler controller logic: only needs to manage CRDs, not interact with proxy runtime APIs.  
* Strong observability and status reporting via Gateway API metrics and Proxy metrics.

Cons::
* Each `Gateway` supports ≤ 64 listeners, additional listeners require `XListenerSet`, which can add management overhead.
* Still constrained by cloud LoadBalancer listener caps (10-100), necessitating sharding at scale.
* Slightly heavier footprint than a plain HAProxy setup.

References::
* https://gateway-api.sigs.k8s.io/[Kubernetes Gateway API]
* https://gateway-api.sigs.k8s.io/geps/gep-1713/?h=listenerset[Gateway API GEP-1713 XListenerSet]
* https://gateway.envoyproxy.io/[Envoy Gateway]
* https://gateway.envoyproxy.io/v1.1/concepts/concepts_overview/[Envoy Gateway concepts]
* https://docs.cilium.io/en/stable/network/servicemesh/gateway-api/gateway-api/#gs-gateway-api/[Cilium Gateway API]
* https://kgateway.dev/[KGateway]
* https://community.exoscale.com/product/networking/nlb/overview/#limitations[Exoscale limitations]
* https://github.com/exoscale/exoscale-cloud-controller-manager/blob/master/docs/service-loadbalancer.md[CCM Service LoadBalancer]

== Decision

We will adopt Option 4, a **Gateway API based architecture** for multiplexing SSH traffic across Codey customer instances.

This design leverages the Kubernetes Gateway API as the declarative control plane for dynamic TCP routing, while allowing flexibility in choosing the underlying implementation such as Envoy Gateway, Cilium Gateway API, KGateway etc.

The custom controller will manage Gateway API resources (`Gateway`, `TCPRoute`, `ReferenceGrant`, and optionally `XListenerSet`) to dynamically assign ports and routes for each tenant.  
The selected Gateway implementation (Envoy, Cilium, etc.) will reconcile these CRDs into live data-plane configuration without requiring reloads or manual synchronization.

.Sharding model:
  - Each shard = one `Gateway` object behind its own `Service type=LoadBalancer` (one IP).
  - Shard capacity is provider-specific (Cloudscale = 100 listeners/IP, Exoscale = 10 listeners/IP).
  - The controller allocates ports from a reserved per-shard ranges (sticky per tenant to avoid collisions) and updates each tenant's Forgejo configuration (SSH_DOMAIN, SSH_PORT)
  - DNS uses deterministic hostnames: `ssh1.codey.ch`, `ssh2.codey.ch`, each resolving to a shard IP.

.Rationale:
  - Gateway API is the next generation Kubernetes Ingress, ensuring long-term compatibility and portability.
  - Multiple conformant implementations exist, providing choice and ecosystem flexibility.
  - Dynamic listener and route updates occur natively through the Gateway controller, no reloads, no direct API integration required.
  - The controller logic remains simple: declarative CRD management and port allocation rather than low-level proxy configuration.

.Note on complexity:
  - LoadBalancer sharding remains necessary due to provider listener caps (10-100 listeners/IP).  
  - This complexity is isolated to the controller layer (shard lifecycle, DNS updates), not the data plane.
  - The architecture remains portable across providers and Gateway implementations.

== Implementation Notes

* Each Forgejo instance listens internally on port `2222`.
* Controller maintains a CRD tracking shard capacity, port ranges, and assignments.
* When a shard reaches capacity, the controller provisions a new `Gateway` and assigns the next DNS alias.
* NetworkPolicies allow traffic from the proxy namespace to tenant namespaces on `TCP/2222`.
* Health, metrics, and reconciliation loops ensure the proxy state matches the controller's record.

=== Forgejo Configuration Example

[source,shell]
----
DISABLE_SSH=false
START_SSH_SERVER=true
SSH_LISTEN_PORT=2222
SSH_DOMAIN=ssh2.codey.ch     # shard domain
SSH_PORT=22037               # unique external port
SSH_SERVER_USE_PROXY_PROTOCOL=false
----

== Consequences

* **Cloudscale:** one IP supports ~100 tenants.
* **Exoscale:** one IP supports 10 tenants.
* Uniform controller logic across providers, only capacities differ.
* LoadBalancer costs grow linearly with shard count.

== Risks and Mitigations

* **Hard provider caps:** unavoidable -> automatic sharding.
* **Gateway listener scaling:** if using Gateway API, use `XListenerSet`, enforce per-shard and per-Gateway limits.
* **Port collisions:** non-overlapping port ranges per shard, lease-based allocator.
* **DNS complexity:** deterministic shard hostnames (`sshN.codey.ch`) and automation.
* **Operational drift:** periodic reconcile verifies proxy state against controller CRDs.

